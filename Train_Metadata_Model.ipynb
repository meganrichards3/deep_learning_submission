{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a in [\"2\", \"4\", \"8\", \"16\", \"32\", \"64\", \"128\"]:\n",
    "#   for b in [\"2\", \"4\", \"8\", \"16\", \"32\", \"64\", \"128\"]:\n",
    "#     if a !=b:\n",
    "#       if os.path.exists(\"kl_diffs/ld\" + a + \"_vs_ld\" + b + \"/diff.pt\"):\n",
    "#         print(f\"ld{a} vs ld{b} already exists, skipping\")\n",
    "#       else:\n",
    "#         print(f\"Running {a} vs {b}\")\n",
    "#         gen_kl_diff(m1_path=f\"ld{a}_e9.pt\", m2_path=f\"ld{b}_e9.pt\", embed=True, embed_model_name=\"google/vit-base-patch16-224\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torchvision.utils import save_image\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoFeatureExtractor, AutoModel, ViTImageProcessor\n",
    "\n",
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "def get_embeddings(model, extractor, imgs, device):\n",
    "    inputs = extractor(\n",
    "        images=[x.detach().cpu().numpy() for x in imgs], return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    model.to(device)\n",
    "    embeds = model(**inputs).last_hidden_state[:, 0].to(device)\n",
    "    return embeds\n",
    "\n",
    "def gen_kl_diff(\n",
    "    m1_path: str=None,\n",
    "    m2_path: str=None,\n",
    "    bsize: int=100,\n",
    "    n_kl_samples: int=1000,\n",
    "    n_ll_samples: int=100,\n",
    "    embed: bool=True,\n",
    "    embed_model_name: str=\"google/vit-base-patch16-224\",\n",
    "):\n",
    "    with torch.no_grad():\n",
    "        if embed:\n",
    "            extractor = ViTImageProcessor.from_pretrained(embed_model_name, do_rescale=False)\n",
    "            embed_model = AutoModel.from_pretrained(embed_model_name, output_hidden_states=True)\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        m1 = torch.load(m1_path, weights_only=False)\n",
    "        m1.to(device).eval()\n",
    "\n",
    "        m2 = torch.load(\n",
    "            os.path.join(m2_path),\n",
    "            weights_only=False\n",
    "        )\n",
    "        m2.to(device).eval()\n",
    "        print(device)\n",
    "\n",
    "        # File Structure\n",
    "        m1_name = os.path.basename(m1_path)[:-6]\n",
    "        m2_name = os.path.basename(m2_path)[:-6]\n",
    "\n",
    "        save_dir = 'kl_diffs/' + m1_name + '_vs_' + m2_name\n",
    "        Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "        kl_diffs = []\n",
    "        m1_full_samples = []\n",
    "        m2_full_samples = []\n",
    "\n",
    "        nbatches = n_kl_samples // bsize\n",
    "\n",
    "        for i in tqdm(range(nbatches), total=nbatches):\n",
    "\n",
    "            m1_samples, m1_m1_ll = m1.sample(bsize, device)\n",
    "            m2_samples, m2_m2_ll = m2.sample(bsize, device)\n",
    "\n",
    "            m1_m2_ll = m1.log_likelihood(m2_samples, n_samples=n_ll_samples)\n",
    "            m2_m1_ll = m2.log_likelihood(m1_samples, n_samples=n_ll_samples)\n",
    "\n",
    "            kl_diffs.append(-(m1_m1_ll + m1_m2_ll) + (m2_m1_ll + m2_m2_ll))\n",
    "\n",
    "            if embed:\n",
    "                m1_full_samples.append(get_embeddings(embed_model, extractor, m1_samples, device))\n",
    "                m2_full_samples.append(get_embeddings(embed_model, extractor, m2_samples, device))\n",
    "            else:\n",
    "                m1_full_samples.append(m1_samples)\n",
    "                m2_full_samples.append(m2_samples)\n",
    "\n",
    "        kl_diffs = torch.concatenate(kl_diffs)\n",
    "        m1_samples = torch.concatenate(m1_full_samples)\n",
    "        m2_samples = torch.concatenate(m2_full_samples)\n",
    "        samples = torch.concatenate((m1_samples, m2_samples), axis=-1)\n",
    "        torch.save(kl_diffs, save_dir + '/diff.pt')\n",
    "        torch.save(samples, save_dir + '/img.pt')\n",
    "\n",
    "gen_kl_diff(m1_path=\"ld2_e9.pt\", m2_path=\"ld4_e9.pt\", embed=True, embed_model_name=\"google/vit-base-patch16-224\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !zip -r file.zip kl_diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MetaMLP(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dims=None,\n",
    "        activation='relu',\n",
    "    ):\n",
    "        super(MetaMLP, self).__init__()\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [1536, 1000, 1000, 1000, 1]\n",
    "\n",
    "        if activation == 'relu':\n",
    "            act_fn = nn.ReLU()\n",
    "\n",
    "        modules = []\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            modules.append(nn.Linear(hidden_dims[i], hidden_dims[i + 1]))\n",
    "            modules.append(nn.ReLU())\n",
    "\n",
    "        self.model = nn.Sequential(*modules[:-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "class MetaDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.n_per_set = len(self.data[0])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = idx // self.n_per_set\n",
    "\n",
    "        return self.data[idx], self.labels[idx].float() / 1000.\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum([len(d) for d in self.data])\n",
    "\n",
    "\n",
    "def load_data(files, device):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for l in files:\n",
    "\n",
    "        lab = \"kl_diffs/\" +l + \"/diff.pt\"\n",
    "        da =  \"kl_diffs/\" + l + \"/img.pt\"\n",
    "\n",
    "        a = torch.load(lab, weights_only=True, map_location=torch.device(\"cpu\")).to(device)\n",
    "        b = torch.load(da, weights_only=True, map_location=torch.device(\"cpu\")).to(device)\n",
    "\n",
    "        labels.append(a)\n",
    "        data.append(b)\n",
    "\n",
    "    return torch.cat(data, dim=0), torch.cat(labels, dim=0)\n",
    "\n",
    "def split(images, labels, split_ratio=0.8, subset = 1):\n",
    "\n",
    "    if subset != 1:\n",
    "        print(\"WARNING: Training on a subset\")\n",
    "        subset_samples = int(labels.size(0)*subset)\n",
    "        indices = torch.randperm(labels.size(0))\n",
    "        images = images[indices[:subset_samples]]\n",
    "        labels = labels[indices[:subset_samples]]\n",
    "\n",
    "    # Calculate the number of samples to select\n",
    "    num_samples = int(labels.size(0)*subset * split_ratio)\n",
    "\n",
    "    # Randomly select samples for the first split\n",
    "    indices = torch.randperm(labels.size(0))\n",
    "    selected_images = images[indices[:num_samples]]\n",
    "    selected_labels = labels[indices[:num_samples]]\n",
    "\n",
    "    remaining_images = images[indices[num_samples:]]\n",
    "    remaining_labels = labels[indices[num_samples:]]\n",
    "\n",
    "    return (selected_images, selected_labels), (remaining_images, remaining_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ConstantLR, LinearLR, SequentialLR\n",
    "\n",
    "\n",
    "def train(\n",
    "    bsize: int=256,\n",
    "    nepochs: int=10,\n",
    "    lr: float=1e-3,\n",
    "):\n",
    "\n",
    "    model = MetaMLP()\n",
    "    model.cuda()\n",
    "\n",
    "    images, labels = load_data(files = os.listdir(\"kl_diffs\"), device ='cuda')\n",
    "    train, test = split(images, labels, subset=0.5)\n",
    "\n",
    "    trainset = MetaDataset(train[0], train[1])\n",
    "    testset = MetaDataset(test[0], test[1])\n",
    "\n",
    "    trainloader = DataLoader(trainset, batch_size=bsize, shuffle=True)\n",
    "    testloader = DataLoader(testset, batch_size=bsize, shuffle=False)\n",
    "\n",
    "    batch_per_epoch = len(trainloader)\n",
    "    optim = Adam(model.parameters(), lr=lr)\n",
    "    scheduler = SequentialLR(\n",
    "        optim,\n",
    "        schedulers=[\n",
    "            LinearLR(optim, start_factor=1e-3, end_factor=1, total_iters=batch_per_epoch),\n",
    "            ConstantLR(optim, factor=1, total_iters=(nepochs - 2) * batch_per_epoch),\n",
    "            LinearLR(optim, start_factor=1, end_factor=1e-3, total_iters=batch_per_epoch),\n",
    "        ],\n",
    "        milestones=[batch_per_epoch, (nepochs - 1) * batch_per_epoch],\n",
    "    )\n",
    "    optim.zero_grad()\n",
    "\n",
    "    for epoch in range(nepochs):\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        for i, (imgs, labels) in tqdm(enumerate(trainloader), total=len(trainloader)):\n",
    "            out = model(imgs).squeeze()\n",
    "            loss = ((out - labels)**2).mean()\n",
    "            #loss = F.binary_cross_entropy_with_logits(out, labels)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "            scheduler.step()\n",
    "            train_loss += loss.item()\n",
    "            train_acc += ((out > 0) == (labels > 0)).float().mean().item()\n",
    "\n",
    "        train_loss /= len(trainloader)\n",
    "        train_acc /= len(trainloader)\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f}\\tAccuracy: {train_acc:.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        test_loss = 0\n",
    "        test_acc = 0\n",
    "        for i, (imgs, labels) in tqdm(enumerate(testloader), total=len(testloader)):\n",
    "            out = model(imgs).squeeze()\n",
    "            loss = ((out - labels)**2).mean()\n",
    "            #loss = F.binary_cross_entropy_with_logits(out, labels)\n",
    "            test_loss += loss.item()\n",
    "            test_acc += ((out > 0) == (labels > 0)).float().mean().item()\n",
    "\n",
    "        test_loss /= len(testloader)\n",
    "        test_acc /= len(testloader)\n",
    "\n",
    "        print(f\"Test Loss: {test_loss:.4f}\\tAccuracy: {test_acc:.4f}\")\n",
    "\n",
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
